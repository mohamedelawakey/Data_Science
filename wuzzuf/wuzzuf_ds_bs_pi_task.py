# -*- coding: utf-8 -*-
"""wuzzuf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wRq9kVhPB-W6qpyrjz_yN6cLoV8VjbkX
"""

from bs4 import BeautifulSoup
import pandas as pd
import requests
import time

search_list = ["Machine Learning Engineer", "Data Analyst", "Data Scientist", "Business Intelligence"]
base_url = "https://wuzzuf.net/search/jobs/?q={}&a=hpb&start={}"
headers = {"User-Agent": "Mozilla/5.0"}

all_jobs_list = []

for job_title in search_list:
    for page in range(0, 16):
        url = base_url.format(job_title.replace(" ", "%20"), page)
        print(f"Scraping for page {page}: {url}, the job title is : {job_title}")

        response = requests.get(url, headers = headers)
        soup = BeautifulSoup(response.content, 'lxml')

        titles = soup.find_all("h2", {'class': 'css-m604qf'})
        companies = soup.find_all('a', {'class': 'css-17s97q8'})
        occupations = soup.find_all('div', {'class': 'css-1lh32fc'})
        experiences = soup.find_all('div', {'class': 'css-y4udm8'})
        companies_locations = soup.find_all('span', {'class': 'css-5wys0k'})

        title_list = [title.text.strip() for title in titles]
        links_list = ["https://wuzzuf.net" + title.a['href'] for title in titles]
        companies_list = [company.text.replace(' -', '') for company in companies]
        occupation_list = [occupation.text[:9] + ' / ' + occupation.text[9:] for occupation in occupations]
        exp_list = [exp.text for exp in experiences]
        companies_locations_list = [loc.text for loc in companies_locations]

        for i in range(min(len(title_list), len(links_list), len(companies_list), len(occupation_list), len(exp_list), len(companies_locations_list))):
          job_data = {
              'Title': title_list[i],
              'Link': links_list[i],
              'Company': companies_list[i],
              'Company Location': companies_locations_list[i],
              'Occupation': occupation_list[i],
              'Experience': exp_list[i]
          }
          all_jobs_list.append(job_data)

        time.sleep(1)

data = pd.DataFrame(all_jobs_list)
data.to_csv("wuzzuf_scraped.csv", index=False)

data

